import requests # ğŸŒ ì›¹ í˜ì´ì§€ ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from bs4 import BeautifulSoup # ğŸ“„ HTML íŒŒì‹±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re # ğŸ” ì •ê·œ í‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from typing import Dict, Optional

# ğŸ‘¤ ê¸°ë³¸ User-Agent: ì›¹ ì„œë²„ì— ìš”ì²­ ì‹œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ì—¬ ì°¨ë‹¨ì„ í”¼í•˜ëŠ” ë° ë„ì›€
DEFAULT_USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'

def clean_content(text: Optional[str]) -> str:
    """
    í¬ë¡¤ë§ëœ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì œê±°í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤. ğŸ§¹
    - HTML ì£¼ì„, URL, ê´‘ê³  ë¬¸êµ¬, ê¸°ì ì •ë³´ ë“±ì„ ì œê±°í•©ë‹ˆë‹¤.
    Args:
        text (Optional[str]): ì •ë¦¬í•  ì›ë³¸ í…ìŠ¤íŠ¸.
    Returns:
        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸.
    """
    if not text:
        return ""
    
    # ğŸ—‘ï¸ ì œê±°í•  íŒ¨í„´ ëª©ë¡ (ì •ê·œì‹) ê³¼ ì„¤ëª…:
    # HTML ì£¼ì„ ì œê±° ()
    text = re.sub(r'', '', text, flags=re.DOTALL) # flags=re.DOTALL: ì—¬ëŸ¬ ì¤„ì— ê±¸ì¹œ ì£¼ì„ë„ ì²˜ë¦¬
    # ì¼ë°˜ì ì¸ URL íŒ¨í„´ ì œê±° (ì£¼ì˜: ê¸°ì‚¬ ë‚´ ìœ ì˜ë¯¸í•œ ë§í¬ë„ ì œê±°ë  ìˆ˜ ìˆìŒ)
    text = re.sub(r'http[s]?://\S+', '', text)
    # ê´‘ê³  ë¬¸êµ¬ ì œê±° (ì˜ˆ: [ê´‘ê³ ], (ê´‘ê³ ))
    text = re.sub(r'\[.*?ê´‘ê³ .*?\]', '', text, flags=re.IGNORECASE) # IGNORECASE: ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
    text = re.sub(r'\(.*?ê´‘ê³ .*?\)', '', text, flags=re.IGNORECASE)
    # ê¸°ì ì •ë³´ íŒ¨í„´ ì œê±° (ì˜ˆ: OOO ê¸°ì (email@example.com), OOO ê¸°ì email@example.com)
    text = re.sub(r'[\w\s]+ê¸°ì\s*(?:\([\w@.]+\)|[\w@.]+)?', '', text) 
    # ìˆœìˆ˜ ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)
    # ì´ë¯¸ì§€/ìë£Œ ì¶œì²˜ í‘œì‹œ ì œê±° (ì˜ˆ: [ì‚¬ì§„=ì—°í•©ë‰´ìŠ¤], (ìë£Œì œê³µ=OOO))
    text = re.sub(r'[\[\(]?\s*(ì‚¬ì§„|ìë£Œ|ì´ë¯¸ì§€)\s*[:=]\s*[^\]\)]+\s*[\]\)]?', '', text, flags=re.IGNORECASE)
    text = re.sub(r'/\s*(ì‚¬ì§„|ìë£Œ|ì´ë¯¸ì§€)\s*[:=]\s*\S+', '', text, flags=re.IGNORECASE) # / ì‚¬ì§„=OOO í˜•ì‹
    # ê¸°íƒ€ ë¶ˆí•„ìš” ë¬¸êµ¬ (ì˜ˆ: â–¶ ê´€ë ¨ê¸°ì‚¬, â“’ ë‰´ìŠ¤1ì½”ë¦¬ì•„, ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€ ë“±)
    text = re.sub(r'â–¶\s*ê´€ë ¨\s*ê¸°ì‚¬\s*.*', '', text) # "â–¶ ê´€ë ¨ê¸°ì‚¬ ì–´ì©Œê³ ì €ì©Œê³ " ë¶€ë¶„ ì œê±°
    text = re.sub(r'â“’\s*[\w\s]+(?:ë‰´ìŠ¤|ì‹ ë¬¸|ë¯¸ë””ì–´|ì¼ë³´|ê²½ì œ|ë°©ì†¡)\s*(?:ì½”ë¦¬ì•„|íŠ¹íŒŒì›)?\s*(?:ë¬´ë‹¨ì „ì¬\s*ë°\s*ì¬ë°°í¬\s*ê¸ˆì§€)?', '', text, flags=re.IGNORECASE) # ì €ì‘ê¶Œ ì •ë³´
    text = re.sub(r'ë¬´ë‹¨\s*ì „ì¬\s*ë°\s*ì¬ë°°í¬\s*ê¸ˆì§€', '', text, flags=re.IGNORECASE) # "ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€" ë¬¸êµ¬
    # ê°œí–‰(ì—”í„°), íƒ­ì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½
    text = re.sub(r'[\r\n\t]+', ' ', text)
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¶•ì†Œ
    text = re.sub(r'\s{2,}', ' ', text)
    
    return text.strip() # ì–‘ ë ê³µë°± ìµœì¢… ì œê±°

def fetch_html(url: str) -> Optional[str]:
    """
    ì£¼ì–´ì§„ URLì˜ HTML ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ğŸŒ
    Args:
        url (str): ê°€ì ¸ì˜¬ ì›¹ í˜ì´ì§€ì˜ URL.
    Returns:
        Optional[str]: HTML ë‚´ìš© ë¬¸ìì—´. ì‹¤íŒ¨ ì‹œ None.
    """
    try:
        headers = {'User-Agent': DEFAULT_USER_AGENT} # ğŸ•µï¸â€â™‚ï¸ User-Agent ì„¤ì •
        # íƒ€ì„ì•„ì›ƒ(10ì´ˆ) ì„¤ì •í•˜ì—¬ ë¬´í•œì • ê¸°ë‹¤ë¦¬ëŠ” ê²ƒ ë°©ì§€
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # HTTP ì˜¤ë¥˜(4xx, 5xx) ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ì¼ìœ¼í‚´

        # ğŸ”„ ì¸ì½”ë”© ì²˜ë¦¬: ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ê°€ ê°€ë” ISO-8859-1ë¡œ ì˜ëª» ê°ì§€ë˜ëŠ” ê²½ìš° UTF-8ë¡œ ì‹œë„
        if response.encoding and response.encoding.lower() == 'iso-8859-1':
            # response.apparent_encoding (ë‚´ìš© ê¸°ë°˜ ì¶”ì¸¡)ì´ 'iso-8859-1'ì´ ì•„ë‹ˆë©´ ê·¸ê²ƒì„ ì‚¬ìš©,
            # ê·¸ê²ƒë§ˆì € 'iso-8859-1'ì´ë©´ 'utf-8'ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
            final_encoding = response.apparent_encoding if response.apparent_encoding and response.apparent_encoding.lower() != 'iso-8859-1' else 'utf-8'
            response.encoding = final_encoding
        
        return response.text # HTML ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜
    except requests.exceptions.RequestException as e: # ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"[Crawler] ğŸ› URL ìš”ì²­ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
        return None
    except Exception as e: # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"[Crawler] ğŸ’¥ HTML ê°€ì ¸ì˜¤ê¸° ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {e}")
        return None

def parse_news_data(html_content: str) -> Dict[str, str]:
    """
    HTML ë‚´ìš©ì—ì„œ ë‰´ìŠ¤ ì œëª©, ì‘ì„± ë‚ ì§œ/ì‹œê°„, ë³¸ë¬¸, ì–¸ë¡ ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ğŸ§©
    BeautifulSoupë¥¼ ì‚¬ìš©í•˜ì—¬ HTMLì„ íŒŒì‹±í•˜ê³ , ë‹¤ì–‘í•œ CSS ì„ íƒìë¥¼ ì‹œë„í•˜ì—¬ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    Args:
        html_content (str): íŒŒì‹±í•  HTML ë‚´ìš©.
    Returns:
        Dict[str, str]: ì¶”ì¶œëœ ë‰´ìŠ¤ ì •ë³´ (ì œëª©, ì‘ì„±ë‚ ì§œ ë° ì‹œê°„, ë³¸ë¬¸, ì–¸ë¡ ì‚¬). ì˜¤ë¥˜ ì‹œ 'error' í‚¤ í¬í•¨.
    """
    if not html_content: # HTML ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
        return {
            "ì œëª©": "HTML ë‚´ìš© ì—†ìŒ", "ì‘ì„±ë‚ ì§œ ë° ì‹œê°„": "ë‚ ì§œ ì—†ìŒ",
            "ë³¸ë¬¸": "HTML ë‚´ìš©ì´ ì—†ì–´ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ì–¸ë¡ ì‚¬": "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ",
            "error": "HTML ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        }

    soup = BeautifulSoup(html_content, 'html.parser') # ğŸœ HTML íŒŒì„œ ì¤€ë¹„
    
    # ğŸ“Œ ë‹¤ì–‘í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ êµ¬ì¡°ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ CSS ì„ íƒì ëª©ë¡
    # ê° í•­ëª©(ì œëª©, ë‚ ì§œ ë“±)ì— ëŒ€í•´ ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‹œë„í•©ë‹ˆë‹¤.

    # ğŸ“° ì œëª© (Title)
    title_selectors = [
        'h2.media_end_head_headline', # ì£¼ìš” í—¤ë“œë¼ì¸ ì„ íƒì
        'h2#title_area span',         # ë‹¤ë¥¸ ì œëª© ì˜ì—­ì˜ span íƒœê·¸
        'div.news_headline h4',       # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ë‚´ë¶€ h4 íƒœê·¸
        '#ct h1.title'                # ì»¨í…ì¸  ì˜ì—­ì˜ h1.title (êµ¬ë²„ì „ ë˜ëŠ” ë‹¤ë¥¸ ë ˆì´ì•„ì›ƒ)
    ]
    title_tag = None
    for selector in title_selectors:
        title_tag = soup.select_one(selector)
        if title_tag: break
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    # ğŸ“… ì‘ì„± ë‚ ì§œ ë° ì‹œê°„ (Date and Time)
    date_selectors = [
        'span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME', # ê°€ì¥ ì •í™•í•œ ì •ë³´ (data-date-time ì†ì„±)
        'div.article_info span.author em',                           # êµ¬ë²„ì „ ë ˆì´ì•„ì›ƒ
        'span.article_info span.date',                               # ì¼ë°˜ì ì¸ ë‚ ì§œ í´ë˜ìŠ¤
        'div.info span.time',                                        # ì •ë³´ ì˜ì—­ì˜ ì‹œê°„
        'div.news_info span.time',                                   # ë‰´ìŠ¤ ì •ë³´ ì˜ì—­ì˜ ì‹œê°„
        'span.date'                                                  # ê°€ì¥ ì¼ë°˜ì ì¸ ë‚ ì§œ íƒœê·¸ (ìµœí›„ì˜ ìˆ˜ë‹¨)
    ]
    date_tag = None
    for selector in date_selectors:
        date_tag = soup.select_one(selector)
        if date_tag: break
        
    date_text = "ë‚ ì§œ ì—†ìŒ"
    if date_tag:
        if date_tag.has_attr('data-date-time'): # ì„ í˜¸: 'YYYY-MM-DD HH:MM:SS' í˜•ì‹
            date_text = date_tag['data-date-time']
        elif date_tag.has_attr('data-modify-date-time'): # ì°¨ì„ : ìˆ˜ì • ì‹œê°„ì´ë¼ë„ ì‚¬ìš©
             date_text = date_tag['data-modify-date-time']
        else: # í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ë° ì •ë¦¬
            date_text = date_tag.get_text(strip=True)
            # "ì…ë ¥ yy.MM.DD. ì˜¤ì „/ì˜¤í›„ H:MM" ë˜ëŠ” "ìˆ˜ì • ..." ê³¼ ê°™ì€ íŒ¨í„´ì—ì„œ ì•ë¶€ë¶„ ì œê±°
            date_text = re.sub(r'^(ì…ë ¥|ìˆ˜ì •)\s*', '', date_text).strip()


    # ğŸ“„ ë³¸ë¬¸ (Content) - ê°€ì¥ ì¤‘ìš”!
    content_selectors = [
        'article#dic_area',          # ë„¤ì´ë²„ ë‰´ìŠ¤ ì£¼ìš” ë³¸ë¬¸ ID (ê°€ì¥ ìš°ì„ )
        'div#articleBodyContents',   # ë‹¤ë¥¸ ì£¼ìš” ë³¸ë¬¸ ID
        'div.article_body',          # ì¼ë°˜ì ì¸ ë³¸ë¬¸ í´ë˜ìŠ¤
        'div#newsct_article',        # ë‰´ìŠ¤ ì»¨í…ì¸  ì•„í‹°í´
        'div.news_end_content',      # ë‰´ìŠ¤ ëë¶€ë¶„ ì»¨í…ì¸  (ë‹¤ë¥¸ êµ¬ì¡°ì¼ ê²½ìš°)
        'section.article_content'    # ì„¹ì…˜ íƒœê·¸ ì•ˆì˜ ì•„í‹°í´ ì»¨í…ì¸ 
    ]
    content_tag = None
    for selector in content_selectors:
        content_tag = soup.select_one(selector)
        if content_tag: break
    
    raw_content = ""
    if content_tag:
        # ë³¸ë¬¸ ë‚´ ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³ , ëŒ“ê¸€ ì˜ì—­ ë“±) ë¯¸ë¦¬ ì œê±°
        elements_to_remove_selectors = [
            'script', 'style', 'iframe', '.ad_area', '.da_obj', '.vod_player', 
            '.u_rmcplayer', 'figure.nbd_imput_class', '.promotion_area', 
            '.link_news', '.news_end_btn_wrap', '.reporter_area', '.copyright',
            'div.supporter_layer_01', 'div.news_like_block', 
            'div.u_cbox',  # ëŒ“ê¸€ ì˜ì—­ (ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€)
            'div.media_end_linked_news_box', 'div.media_end_channel_subscribe',
            'div.media_end_bottom_sticky_ad', 'div.NCS_AD' # ê°ì¢… ê´‘ê³  ì˜ì—­
        ]
        for selector_to_remove in elements_to_remove_selectors:
            for unwanted_tag in content_tag.select(selector_to_remove): # ì„ íƒìë¡œ ì°¾ì•„ì„œ
                unwanted_tag.decompose() # ğŸŒ³ í•´ë‹¹ íƒœê·¸ì™€ ê·¸ ìì‹ë“¤ ëª¨ë‘ ì œê±°
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ìì‹ íƒœê·¸ë“¤ì˜ í…ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ í•©ì¹¨)
        raw_content = content_tag.get_text(separator=' ', strip=True) 
    
    cleaned_body = clean_content(raw_content) if raw_content else "ë³¸ë¬¸ ì—†ìŒ"
    # ì •ë¦¬ í›„ì—ë„ ë‚´ìš©ì´ ì—†ë‹¤ë©´, ì›ë³¸ì— ë¬¸ì œê°€ ìˆì—ˆìŒì„ ëª…ì‹œ
    if not cleaned_body.strip() and raw_content:
        cleaned_body = "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí–ˆìœ¼ë‚˜, ì •ë¦¬ ê³¼ì •ì—ì„œ ëª¨ë‘ ì œê±°ë˜ì—ˆê±°ë‚˜ ìœ íš¨í•œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."


    # ğŸ¢ ì–¸ë¡ ì‚¬ (Press/Media Company)
    press_selectors = [
        'span.media_end_head_top_logo_name_text', # ì„ í˜¸í•˜ëŠ” í…ìŠ¤íŠ¸ ë¡œê³ 
        'a.media_end_head_top_logo img[alt]',     # ì´ë¯¸ì§€ ë¡œê³ ì˜ alt ì†ì„± (alt ì†ì„±ì´ ìˆëŠ” ê²½ìš°ë§Œ)
        'div.press_logo img[alt]',                # ë‹¤ë¥¸ ì´ë¯¸ì§€ ë¡œê³ ì˜ alt ì†ì„±
        'div.organization_info a.link_media',     # ê¸°ê´€ ì •ë³´ ë‚´ ë§í¬
        'span.ofhd_float_title_text_press',       # íŠ¹ì • ë ˆì´ì•„ì›ƒì˜ ì–¸ë¡ ì‚¬ëª…
        'em.media_summary_company'                # ìš”ì•½ ì •ë³´ì˜ íšŒì‚¬ëª…
    ]
    press_tag = None
    press_name = "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"

    for selector in press_selectors:
        press_tag = soup.select_one(selector)
        if press_tag:
            if press_tag.name == 'img' and press_tag.has_attr('alt'):
                press_name = press_tag['alt'].strip() # ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸ ì‚¬ìš©
            else:
                press_name = press_tag.get_text(strip=True) # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‚¬ìš©
            
            # "ì–¸ë¡ ì‚¬ ì„ ì •" ê³¼ ê°™ì€ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
            press_name = press_name.replace("ì–¸ë¡ ì‚¬ ì„ ì •", "").strip()
            if press_name: # ìœ íš¨í•œ ì´ë¦„ì´ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨
                break 
        if press_name and press_name != "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ": break # í˜¹ì‹œ ë¹ˆ ë¬¸ìì—´ì´ ë  ê²½ìš° ëŒ€ë¹„

    # ìµœí›„ì˜ ìˆ˜ë‹¨: meta íƒœê·¸ì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì‹œë„
    if not press_name or press_name == "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ":
        meta_press_selectors = [
            "meta[property='og:site_name']", # Open Graph ì‚¬ì´íŠ¸ ì´ë¦„
            "meta[name='twitter:site']"      # Twitter ì¹´ë“œ ì‚¬ì´íŠ¸ ì´ë¦„
        ]
        for meta_selector in meta_press_selectors:
            meta_tag = soup.select_one(meta_selector)
            if meta_tag and meta_tag.has_attr('content'):
                press_name = meta_tag['content'].strip()
                if press_name: break


    # ì œëª©ì´ë‚˜ ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ë¡œ ê°„ì£¼
    if title == "ì œëª© ì—†ìŒ" and cleaned_body == "ë³¸ë¬¸ ì—†ìŒ":
        return {
            "ì œëª©": title, "ì‘ì„±ë‚ ì§œ ë° ì‹œê°„": date_text, "ë³¸ë¬¸": cleaned_body, "ì–¸ë¡ ì‚¬": press_name,
            "error": "ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ì„ ëª¨ë‘ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ê°€ ë‹¤ë¥´ê±°ë‚˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        }

    return { # ğŸ ìµœì¢… ì¶”ì¶œ ê²°ê³¼
        "ì œëª©": title,
        "ì‘ì„±ë‚ ì§œ ë° ì‹œê°„": date_text,
        "ë³¸ë¬¸": cleaned_body,
        "ì–¸ë¡ ì‚¬": press_name
    }

def extract_news_data(url: str) -> Dict[str, str]:
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ğŸš€
    HTMLì„ ê°€ì ¸ì˜¤ê³ , íŒŒì‹±í•˜ì—¬ ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Args:
        url (str): í¬ë¡¤ë§í•  ë‰´ìŠ¤ì˜ URL.
    Returns:
        Dict[str, str]: ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„°. ì‹¤íŒ¨ ì‹œ 'error' í‚¤ë¥¼ í¬í•¨.
    """
    print(f"[Crawler] ğŸ” ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ ì‹œë„: {url}")
    html_content = fetch_html(url) # 1. HTML ê°€ì ¸ì˜¤ê¸°
    if not html_content:
        return {"error": "URLë¡œë¶€í„° HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "ì œëª©": "í¬ë¡¤ë§ ì‹¤íŒ¨", "ë³¸ë¬¸": ""}
    
    news_data = parse_news_data(html_content) # 2. HTML íŒŒì‹±í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
    
    # âš ï¸ ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì„ ê²½ìš° ê²½ê³  ì¶”ê°€ (ì˜¤ë¥˜ëŠ” ì•„ë‹ˆì§€ë§Œ ì£¼ì˜ í•„ìš”)
    if not news_data.get("error") and (not news_data.get("ë³¸ë¬¸") or len(news_data.get("ë³¸ë¬¸")) < 30):
        warning_msg = "ì¶”ì¶œëœ ë³¸ë¬¸ ë‚´ìš©ì´ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤ (30ì ë¯¸ë§Œ). í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        news_data["warning"] = warning_msg # ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
        if not news_data.get("ë³¸ë¬¸"): # ë³¸ë¬¸ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
             news_data["ë³¸ë¬¸"] = "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤."
             # ì´ ê²½ìš° 'error'ë¡œë„ ê°„ì£¼í•  ìˆ˜ ìˆìŒ (ì•± ë¡œì§ì— ë”°ë¼)
             if not news_data.get("error"): # ê¸°ì¡´ ì—ëŸ¬ê°€ ì—†ë‹¤ë©´, ì´ ìƒí™©ì„ ì—ëŸ¬ë¡œ ì„¤ì •
                 news_data["error"] = "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤."
        print(f"[Crawler] âš ï¸ ê²½ê³ : {warning_msg} (URL: {url})")


    # ìµœì¢… ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    if news_data.get("error"):
         print(f"[Crawler] âŒ ë‰´ìŠ¤ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {news_data.get('error')}")
    elif news_data.get("warning"):
         print(f"[Crawler] âš ï¸ ë‰´ìŠ¤ ë°ì´í„° íŒŒì‹± ê²½ê³ : {url}, ê²½ê³ : {news_data.get('warning')}")
    else:
        print(f"[Crawler] âœ… ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: '{news_data.get('ì œëª©', 'ì œëª© ì—†ìŒ')[:30]}...' (ì–¸ë¡ ì‚¬: {news_data.get('ì–¸ë¡ ì‚¬', 'ì •ë³´ì—†ìŒ')})")
        
    return news_data

# Flask ëª¨ë“ˆë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ, if __name__ == '__main__': ë¶€ë¶„ì€ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°í•©ë‹ˆë‹¤.
# í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ì‹œë©´ ë³„ë„ì˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì´ ëª¨ë“ˆì„ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
# ì˜ˆì‹œ:
# if __name__ == '__main__':
#     test_url = "ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ë„¤ì´ë²„ ë‰´ìŠ¤ URLì„ ì…ë ¥í•˜ì„¸ìš”"
#     data = extract_news_data(test_url)
#     import json
#     print(json.dumps(data, ensure_ascii=False, indent=2))