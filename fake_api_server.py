import json
import re
import uuid # ğŸ†” ê³ ìœ  ID ìƒì„±ì„ ìœ„í•¨
import time # â° ìƒì„± ì‹œê°„ ê¸°ë¡ì„ ìœ„í•¨
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field # âš™ï¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ìŠ¤í‚¤ë§ˆ ì •ì˜
from typing import Dict, Optional, Tuple, List, Union
from llama_cpp import Llama # ğŸ¦™ Llama ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- âš™ï¸ ëª¨ë¸ ì„¤ì • ---
# â— ì¤‘ìš”: MODEL_PATHë¥¼ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¡œ ì •í™•íˆ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_FILENAME = "Meta-Llama-3-8B-Instruct-Q5_K_M.gguf" # ğŸ“„ ì‚¬ìš©í•  ëª¨ë¸ íŒŒì¼ëª…
# ì˜ˆì‹œ: fake_api_server.pyê°€ ìˆëŠ” í´ë”ì˜ ìƒìœ„ í´ë”ì— models í´ë”ê°€ ìˆëŠ” ê²½ìš°
MODEL_PATH = os.path.join(BASE_DIR, "models", MODEL_FILENAME)
# MODEL_PATH = "C:/path/to/your/models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf" # ìœˆë„ìš° ì ˆëŒ€ ê²½ë¡œ ì˜ˆì‹œ

MODEL_API_NAME = "Llama-3-8B-Instruct-NewsAnalyzer" # APIì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
N_GPU_LAYERS = 0  # ğŸ® GPU ì˜¤í”„ë¡œë”© ë ˆì´ì–´ ìˆ˜ (0 = CPUë§Œ ì‚¬ìš©)
N_CTX = 4096      # ğŸ§  ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í† í° ìˆ˜)

app = FastAPI(
    title="Real News Analysis API Server (Llama-3)",
    description=f"{MODEL_API_NAME} ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ğŸ¤–ğŸ“°",
    version="1.1.1"
)

llm_model: Optional[Llama] = None # Llama ëª¨ë¸ ê°ì²´ë¥¼ ì €ì¥í•  ë³€ìˆ˜
if not os.path.exists(MODEL_PATH): # ğŸ§ ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
    print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"BASE_DIR (ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€): {BASE_DIR}")
    print("MODEL_PATH í™˜ê²½ ë³€ìˆ˜ë‚˜ ì½”ë“œ ë‚´ ê²½ë¡œ ì„¤ì •ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”. ğŸ˜¥")
else:
    try:
        print(f"â³ '{MODEL_PATH}'ì—ì„œ Llama-3 ëª¨ë¸ ë¡œë“œ ì¤‘... ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        llm_model = Llama(
            model_path=MODEL_PATH,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=N_CTX,
            chat_format="llama-3", # ëª¨ë¸ì— ë§ëŠ” ì±„íŒ… í˜•ì‹ ì§€ì •
            verbose=True, # ë¡œë“œ ê³¼ì • ìƒì„¸ ì •ë³´ ì¶œë ¥
            use_mmap=False  # â— PrefetchVirtualMemory ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ Falseë¡œ ì„¤ì • (ë©”ëª¨ë¦¬ ë§¤í•‘ ì‚¬ìš© ì•ˆ í•¨)
        )
        print(f"âœ… Llama-3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_API_NAME}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
        llm_model = None

# --- ğŸ“¦ Pydantic ëª¨ë¸ (OpenAI API í˜•ì‹ ëª¨ë°©) ---
# API ìš”ì²­/ì‘ë‹µ ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class ChatMessageInput(BaseModel):
    """ ğŸ’¬ ì±„íŒ… ë©”ì‹œì§€ ì…ë ¥ ëª¨ë¸ """
    role: str # "system", "user", "assistant" ì¤‘ í•˜ë‚˜
    content: str # ë©”ì‹œì§€ ë‚´ìš©

class ChatCompletionRequestReal(BaseModel):
    """ ğŸ¤– ì±„íŒ… ì™„ë£Œ ìš”ì²­ ëª¨ë¸ """
    model: Optional[str] = MODEL_API_NAME # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
    messages: List[ChatMessageInput] # ì±„íŒ… ë©”ì‹œì§€ ëª©ë¡
    temperature: Optional[float] = 0.1 # ğŸŒ¡ï¸ ë‹¤ì–‘ì„± ì œì–´ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì )
    max_tokens: Optional[int] = 1536 #  tokens ìµœëŒ€ ìƒì„± í† í° ìˆ˜
    stream: Optional[bool] = False # ğŸ’¨ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€ (í˜„ì¬ëŠ” Falseë§Œ ì§€ì›)
    top_p: Optional[float] = 0.9 # ğŸ” í™•ë¥  ë¶„í¬ì—ì„œ ìƒìœ„ p%ì˜ í† í°ë§Œ ê³ ë ¤
    stop: Optional[Union[str, List[str]]] = ["\n```", "```\n", "\n\n", "}\n", "}\r\n", "<|eot_id|>"] # ğŸ›‘ ìƒì„± ì¤‘ë‹¨ í† í°

class ResponseMessageOutput(BaseModel):
    """ ğŸ’¬ ì‘ë‹µ ë©”ì‹œì§€ ëª¨ë¸ """
    role: str = "assistant" # ì‘ë‹µ ì—­í• ì€ í•­ìƒ "assistant"
    content: str # ì‘ë‹µ ë‚´ìš© (JSON ë¬¸ìì—´)

class ChatCompletionChoiceOutput(BaseModel):
    """ âœ… ì±„íŒ… ì™„ë£Œ ì„ íƒì§€ ëª¨ë¸ """
    index: int = 0
    message: ResponseMessageOutput
    finish_reason: Optional[str] = "stop" # ì™„ë£Œ ì´ìœ  (ì˜ˆ: "stop", "length")

class ChatCompletionResponseReal(BaseModel):
    """ ğŸ ì±„íŒ… ì™„ë£Œ ì „ì²´ ì‘ë‹µ ëª¨ë¸ """
    id: str = Field(default_factory=lambda: f"chatcmpl-realmodel-{uuid.uuid4().hex}") # ê³ ìœ  ì‘ë‹µ ID
    object: str = "chat.completion" # ê°ì²´ íƒ€ì…
    created: int = Field(default_factory=lambda: int(time.time())) # ìƒì„± íƒ€ì„ìŠ¤íƒ¬í”„
    model: str # ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„
    choices: List[ChatCompletionChoiceOutput] # ì„ íƒì§€ ëª©ë¡ (ì¼ë°˜ì ìœ¼ë¡œ í•˜ë‚˜)

# --- ğŸ“ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ ---
SYSTEM_PROMPT_FOR_MODEL = """ë‹¹ì‹ ì€ ê°€ì§œ ë‰´ìŠ¤ íƒì§€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì¡°ê±´ê³¼ ì¤‘ìš”ë„ì— ë”°ë¼ ê¸°ì‚¬ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
â€» ì¡°ê±´ì´ ì—¬ëŸ¬ ê°œ í•´ë‹¹ë  ê²½ìš°, **ê°€ì¥ ë†’ì€ ì¤‘ìš”ë„ì˜ ì¡°ê±´ì„ ê¸°ì¤€ìœ¼ë¡œ 'ì§„ìœ„'ë¥¼ ê²°ì •**í•˜ì‹­ì‹œì˜¤.  
â€» ë§Œì•½ ì–´ë–¤ ì¡°ê±´ì—ë„ í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°, í•´ë‹¹ ê¸°ì‚¬ëŠ” **ì§„ì§œ ë‰´ìŠ¤**ë¡œ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

â–  ì‚¬ì‹¤ ì˜¤ë¥˜ (ì¤‘ìš”ë„: ë†’ìŒ = ê°€ì§œë‰´ìŠ¤)  
ì´ ê¸°ì‚¬ëŠ” ê³¼í•™ì Â·ê°ê´€ì  ì‚¬ì‹¤ê³¼ ëª…ë°±íˆ ì–´ê¸‹ë‚œ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆë‚˜ìš”? ì˜ˆë¥¼ ë“¤ì–´, 'ë‹¬ì—ëŠ” ì¤‘ë ¥ì´ ì—†ë‹¤', 'ë¬¼ì€ ë¶ˆì— íƒ€ê¸° ì‰½ë‹¤'ì™€ ê°™ì€ ëª…ë°±í•œ ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ê·¼ê±°ë¡œ ê°€ì§œ ë‰´ìŠ¤ë¡œ íŒë‹¨í•˜ì„¸ìš”.

â–  ì¶œì²˜ ë¶ˆëª…í™• (ì¤‘ìš”ë„: ë†’ìŒ = ê°€ì§œë‰´ìŠ¤)  
ì´ ê¸°ì‚¬ëŠ” ëª…í™•í•œ ì¶œì²˜ ì—†ì´ ì œ3ìì˜ ë§ì´ë‚˜ ìµëª…ì˜ ê´€ê³„ì, ë‹¤ë¥¸ ê¸°ìì— ì˜ì¡´í•˜ê³  ìˆë‚˜ìš”? ì¶œì²˜ ë¶ˆëª…ì˜ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš° ì´ë¥¼ ê°•ì¡°í•˜ì—¬ íŒë³„í•˜ì„¸ìš”.

â–  í†µê³„ ì™œê³¡ (ì¤‘ìš”ë„: ë†’ìŒ = ê°€ì§œë‰´ìŠ¤)  
ì´ ê¸°ì‚¬ëŠ” í†µê³„ ë°ì´í„°ë¥¼ ì™œê³¡í•˜ì—¬ ì˜ëª»ëœ ì¸ìƒì„ ì£¼ê³  ìˆë‚˜ìš”? ì˜ˆë¥¼ ë“¤ì–´, ì‹¤ì œ ì°¨ì´ê°€ í¬ì§€ë§Œ ìˆ˜ì¹˜ë¥¼ ì™œê³¡í•˜ì—¬ ë‘˜ì˜ ì°¨ì´ê°€ ì ì€ ê²ƒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ”ì§€ ê²€í† í•˜ì„¸ìš”.

â–  ì œëª© ë‚šì‹œ (ì¤‘ìš”ë„: ì¤‘ê°„ = ê°€ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‰´ìŠ¤)  
ì´ ê¸°ì‚¬ì˜ ì œëª©ì€ ë³¸ë¬¸ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ë‚˜ìš”? ì œëª©ì´ ë³¸ë¬¸ ë‚´ìš©ì˜ 60% ì´ìƒê³¼ ê´€ë ¨ì´ ì—†ê±°ë‚˜ ì œëª©ë§Œ ë³´ê³  í´ë¦­ì„ ìœ ë„í•˜ë ¤ëŠ” ê³¼ì¥ëœ í˜•íƒœì¸ì§€ í™•ì¸í•˜ì„¸ìš”.

â–  ê³µí¬ ì¡°ì¥ (ì¤‘ìš”ë„: ë‚®ìŒ = ê°€ì§œì¼ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë‰´ìŠ¤)  
ì´ ê¸°ì‚¬ëŠ” ì‚¬íšŒì ìœ¼ë¡œ í˜¼ë€ì„ ì•¼ê¸°í•  ìˆ˜ ìˆëŠ” ê³µí¬ê°ì„ ìœ ë°œí•˜ëŠ” í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”? ì˜ˆë¥¼ ë“¤ì–´, 'ìœ„í—˜í•˜ë‹¤', 'ê¸´ê¸‰í•˜ë‹¤', 'ë¶ˆì•ˆí•˜ë‹¤' ë“±ì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë…ìì—ê²Œ ë¶ˆì•ˆê°ì„ ì£¼ëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”.

â–  ê°ì •ì  í‘œí˜„ (ì¤‘ìš”ë„: ë‚®ìŒ = ê°€ì§œì¼ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë‰´ìŠ¤)  
ì´ ê¸°ì‚¬ëŠ” ê°ì„±ì ì¸ í‘œí˜„ì´ë‚˜ ì£¼ê´€ì ì¸ ê²°ë¡ ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”? ì˜ˆë¥¼ ë“¤ì–´, ê°ì •ì  ì–¸ì–´(â€˜ë¶ˆìŒí•˜ë‹¤â€™, â€˜ì–´ì²˜êµ¬ë‹ˆì—†ë‹¤â€™)ë‚˜ ê°•í•˜ê²Œ ê°œì¸ì ì¸ ì˜ê²¬ì„ ì œì‹œí•˜ëŠ” ê²°ë¡ ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ì•„ë˜ ì œê³µë  ë‰´ìŠ¤ ì œëª©ê³¼ ë³¸ë¬¸ì— ëŒ€í•´, ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì‹­ì‹œì˜¤ (ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ JSON ê°ì²´ë§Œ ì¶œë ¥). ì¶œë ¥ì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSONì´ì–´ì•¼ í•©ë‹ˆë‹¤: 
{{
  "ì§„ìœ„": "ì—¬ê¸°ì— íŒë‹¨ ê²°ê³¼ (ì˜ˆ: ê°€ì§œ ë‰´ìŠ¤, ì§„ì§œ ë‰´ìŠ¤ ë“±)",
  "ê·¼ê±°": "ì—¬ê¸°ì— íŒë‹¨ ê·¼ê±° (ì¡°ê±´ì— ë§ëŠ” ì´ìœ , ì—¬ëŸ¬ ê°œì¼ ê²½ìš° ë²ˆí˜¸ ë§¤ê²¨ì„œ)",
  "ë¶„ì„": "ì—¬ê¸°ì— êµ¬ì²´ì ì¸ ë¶„ì„ ë‚´ìš©"
}}
""" # ğŸ§‘â€ğŸ« ëª¨ë¸ì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê³  ì§€ì‹œí•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

def parse_news_data_from_user_message(user_message_content: str) -> Tuple[Optional[str], Optional[str]]:
    """
    USER_MESSAGE_TEMPLATE í˜•ì‹ì˜ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë‰´ìŠ¤ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ğŸ”
    Args:
        user_message_content (str): ì‚¬ìš©ìê°€ ë³´ë‚¸ ë©”ì‹œì§€ ë‚´ìš©.
    Returns:
        Tuple[Optional[str], Optional[str]]: (ë‰´ìŠ¤ ì œëª©, ë‰´ìŠ¤ ë³¸ë¬¸). ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ None ë˜ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜.
    """
    news_title = None
    news_content = None

    # analyzer_api_simulator.pyì˜ USER_MESSAGE_TEMPLATE í˜•ì‹ê³¼ ì¼ì¹˜í•´ì•¼ í•¨:
    # "ë‰´ìŠ¤ ì œëª©: {news_title}\në‰´ìŠ¤ ë³¸ë¬¸: {news_content}\n---\nìœ„ ë‰´ìŠ¤ì— ëŒ€í•´ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."
    title_match = re.search(r"ë‰´ìŠ¤ ì œëª©:\s*(.*?)\në‰´ìŠ¤ ë³¸ë¬¸:", user_message_content, re.DOTALL)
    content_match = re.search(r"ë‰´ìŠ¤ ë³¸ë¬¸:\s*([\s\S]*?)\n---", user_message_content, re.DOTALL)

    if title_match:
        news_title = title_match.group(1).strip()
    if content_match:
        news_content = content_match.group(1).strip()
    
    # â˜ï¸ ë§Œì•½ ìœ„ íŒ¨í„´ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ, ì¢€ ë” ë„“ì€ ë²”ìœ„ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
    if news_content is None and "ë‰´ìŠ¤ ë³¸ë¬¸:" in user_message_content:
        content_start_index = user_message_content.find("ë‰´ìŠ¤ ë³¸ë¬¸:") + len("ë‰´ìŠ¤ ë³¸ë¬¸:")
        # "---" ë§ˆì»¤ ë˜ëŠ” "JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼" ë§ˆì»¤ë¥¼ ì°¾ì•„ ê·¸ ì „ê¹Œì§€ë¥¼ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
        end_marker1 = "\n---"
        end_marker2 = "JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼"
        
        idx1 = user_message_content.find(end_marker1, content_start_index)
        idx2 = user_message_content.find(end_marker2, content_start_index)

        end_idx = -1
        if idx1 != -1 and idx2 != -1: end_idx = min(idx1, idx2)
        elif idx1 != -1: end_idx = idx1
        elif idx2 != -1: end_idx = idx2
            
        if end_idx != -1:
            news_content = user_message_content[content_start_index:end_idx].strip()
        else: # ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ë©”ì‹œì§€ ëê¹Œì§€ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
            news_content = user_message_content[content_start_index:].strip()

    if news_title is None: news_title = "ì œëª© íŒŒì‹± ì‹¤íŒ¨" # ğŸ˜¥ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
    if news_content is None: news_content = "ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨" # ğŸ˜¥ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
    
    return news_title, news_content

def clean_model_output_to_valid_json(model_text_output: str) -> str:
    """ëª¨ë¸ ì¶œë ¥ì„ ìµœëŒ€í•œ ìœ íš¨í•œ JSON ë¬¸ìì—´ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤. âœ¨"""
    text = model_text_output.strip()
    
    # ```json ... ``` ë˜ëŠ” ``` ... ``` ì™€ ê°™ì€ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    if text.startswith("```json"): text = text[len("```json"):].strip()
    elif text.startswith("```"): text = text[len("```"):].strip()
    if text.endswith("```"): text = text[:-len("```")].strip()

    # ì²« ë²ˆì§¸ '{'ì™€ ë§ˆì§€ë§‰ '}'ë¥¼ ì°¾ì•„ ê·¸ ì‚¬ì´ì˜ ë‚´ìš©ì„ JSONìœ¼ë¡œ ê°„ì£¼
    first_brace = text.find('{')
    last_brace = text.rfind('}')
    
    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
        text = text[first_brace : last_brace+1]
    else:
        # JSON ê°ì²´ ê²½ê³„ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì˜¤ë¥˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” JSON ë°˜í™˜
        print(f"[JSON Cleaner] âš ï¸ ëª¨ë¸ ì¶œë ¥ì—ì„œ JSON ê°ì²´ ê²½ê³„ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì¼ë¶€: {model_text_output[:100]}")
        return json.dumps({
            "ì§„ìœ„": "íŒë‹¨ ì˜¤ë¥˜ (ëª¨ë¸ ì‘ë‹µ í˜•ì‹ ë¬¸ì œ)",
            "ê·¼ê±°": f"ëª¨ë¸ì´ ìœ íš¨í•œ JSON ê°ì²´ í˜•íƒœì˜ ë¬¸ìì—´ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ (ì¼ë¶€): {model_text_output[:100]}...",
            "ë¶„ì„": "ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ë‚˜ stop í† í° ì„¤ì •ì„ í™•ì¸í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }, ensure_ascii=False, indent=2) # ensure_ascii=Falseë¡œ í•œê¸€ ìœ ì§€, indentë¡œ ê°€ë…ì„± í™•ë³´
    
    return text.strip()

@app.post("/v1/chat/completions", response_model=ChatCompletionResponseReal)
async def create_real_chat_completion(request: ChatCompletionRequestReal):
    """ ğŸ¤– Llama-3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë‰´ìŠ¤ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ """
    if llm_model is None: # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        raise HTTPException(status_code=503, detail="Llama-3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ğŸ˜¥")

    if not request.messages: # ìš”ì²­ì— ë©”ì‹œì§€ê°€ ì—†ëŠ” ê²½ìš°
        raise HTTPException(status_code=400, detail={"error": {"message": "messages í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."}})

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ê¸° (ë³´í†µ ë§ˆì§€ë§‰ ë©”ì‹œì§€)
    user_message_content = ""
    for msg in request.messages:
        if msg.role == "user":
            user_message_content = msg.content
            break
    
    if not user_message_content: # ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ì—†ëŠ” ê²½ìš°
        raise HTTPException(status_code=400, detail={"error": {"message": "User messageë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}})

    # ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë‰´ìŠ¤ ì œëª©ê³¼ ë³¸ë¬¸ íŒŒì‹±
    news_title, news_content = parse_news_data_from_user_message(user_message_content)

    # íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ë³¸ë¬¸ì´ ì—†ëŠ” ê²½ìš°
    if news_title == "ì œëª© íŒŒì‹± ì‹¤íŒ¨" or news_content == "ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨" or not news_content:
        print(f"[Real API] âš ï¸ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë‰´ìŠ¤ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ë³¸ë¬¸ ì—†ìŒ. ì œëª©: {news_title}, ë‚´ìš© ì¼ë¶€: {news_content[:50] if news_content else 'N/A'}")
        error_json_content = json.dumps({
            "ì§„ìœ„": "íŒë‹¨ ë¶ˆê°€ (ì…ë ¥ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜)",
            "ê·¼ê±°": "í´ë¼ì´ì–¸íŠ¸ê°€ ë³´ë‚¸ ë©”ì‹œì§€ì—ì„œ ë‰´ìŠ¤ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì„ ì •í™•íˆ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.",
            "ë¶„ì„": "ì…ë ¥ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”. (ì˜ˆ: 'ë‰´ìŠ¤ ì œëª©: ... ë‰´ìŠ¤ ë³¸ë¬¸: ... ---')"
        }, ensure_ascii=False, indent=2)
        response_message = ResponseMessageOutput(content=error_json_content)
        choice = ChatCompletionChoiceOutput(message=response_message)
        return ChatCompletionResponseReal(model=request.model or MODEL_API_NAME, choices=[choice])

    print(f"[Real API] ğŸ§  ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ : ì œëª©='{news_title[:30]}...'")

    # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° (ëª¨ë¸ì—ê²Œ ë³´ë‚´ê¸° ì „ì— API ì„œë²„ì—ì„œ íŒë‹¨)
    if len(news_content) < 50: # ğŸ“
        print("[Real API] âš ï¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ (50ì ë¯¸ë§Œ) 'íŒë‹¨ ë¶ˆê°€' JSON ë°˜í™˜.")
        short_content_json = json.dumps({
            "ì§„ìœ„": "íŒë‹¨ ë¶ˆê°€ (ë‚´ìš© ë¶€ì¡±)",
            "ê·¼ê±°": "ì œê³µëœ ë‰´ìŠ¤ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ (50ì ë¯¸ë§Œ) ë¶„ì„ì— í•„ìš”í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "ë¶„ì„": "ë‰´ìŠ¤ ë‚´ìš©ì´ ë¶€ì¡±í•˜ì—¬ ê°€ì§œ ë‰´ìŠ¤ íŒë‹¨ ê¸°ì¤€ì„ ì ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
        }, ensure_ascii=False, indent=2)
        response_message = ResponseMessageOutput(content=short_content_json)
        choice = ChatCompletionChoiceOutput(message=response_message)
        return ChatCompletionResponseReal(model=request.model or MODEL_API_NAME, choices=[choice])

    # í´ë¼ì´ì–¸íŠ¸ê°€ ë³´ë‚¸ ë©”ì‹œì§€ ëª©ë¡ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)ì„ ëª¨ë¸ì— ì „ë‹¬
    final_messages_for_model = [msg.model_dump() for msg in request.messages]
    # analyzer_api_simulator.pyì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì´ë¯¸ êµ¬ì„±í•˜ì—¬ ë³´ë‚´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

    try:
        print(f"[Real API] ğŸš€ Llama-3 ëª¨ë¸ ì¶”ë¡  ì‹œì‘... (max_tokens: {request.max_tokens}, temp: {request.temperature})")
        
        # ğŸ—£ï¸ Llama ëª¨ë¸ í˜¸ì¶œ (OpenAI API v1 í˜¸í™˜ í˜•ì‹ ì‚¬ìš©)
        completion = llm_model.create_chat_completion_openai_v1(
            messages=final_messages_for_model,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p,
            stop=request.stop, # stop í† í° ì„¤ì •
            stream=False, # ìŠ¤íŠ¸ë¦¬ë°ì€ í˜„ì¬ Falseë¡œ ê³ ì •
        )
        
        # ëª¨ë¸ì˜ ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        raw_model_output = completion.choices[0].message.content if completion.choices and completion.choices[0].message else ""
        print(f"[Real API] âœ¨ Llama-3 ëª¨ë¸ ì›ë³¸ ì‘ë‹µ (ì¼ë¶€): {raw_model_output[:250]}...")

        # ëª¨ë¸ ì‘ë‹µì„ ìœ íš¨í•œ JSONìœ¼ë¡œ ì •ë¦¬
        cleaned_json_string = clean_model_output_to_valid_json(raw_model_output)
        
        # ìµœì¢…ì ìœ¼ë¡œ ìœ íš¨í•œ JSONì¸ì§€ ë‹¤ì‹œ íŒŒì‹± ì‹œë„í•˜ì—¬, ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ JSONìœ¼ë¡œ ëŒ€ì²´
        try:
            json.loads(cleaned_json_string) # íŒŒì‹± ì‹œë„ (ì„±ê³µí•˜ë©´ ì•„ë¬´ê²ƒë„ ì•ˆí•¨)
            print("[Real API] âœ… ëª¨ë¸ ì¶œë ¥ì´ ìœ íš¨í•œ JSON í˜•ì‹ì„ì„ í™•ì¸.")
        except json.JSONDecodeError as je:
            print(f"[Real API] âŒ ìµœì¢… ì •ë¦¬ëœ ë¬¸ìì—´ë„ ìœ íš¨í•œ JSONì´ ì•„ë‹˜! ì˜¤ë¥˜: {je}")
            print(f"ì›ë³¸ ëª¨ë¸ ì¶œë ¥: {raw_model_output}")
            print(f"ì •ë¦¬ ì‹œë„í•œ ë¬¸ìì—´: {cleaned_json_string}")
            # ì˜¤ë¥˜ ìƒí™©ì„ ë‚˜íƒ€ë‚´ëŠ” JSONìœ¼ë¡œ ëŒ€ì²´
            cleaned_json_string = json.dumps({
                "ì§„ìœ„": "íŒë‹¨ ì˜¤ë¥˜ (ëª¨ë¸ ì‘ë‹µ í˜•ì‹ ë¬¸ì œ)",
                "ê·¼ê±°": f"ëª¨ë¸ì´ ìœ íš¨í•œ JSONì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ (ì¼ë¶€): {raw_model_output[:100]}...",
                "ë¶„ì„": "ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ë‚˜ ì„¤ì •ì„ í™•ì¸í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜ëŠ” ëª¨ë¸ì´ ìš”ì²­ëœ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ì–´ë ¤ì›Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            }, ensure_ascii=False, indent=2)

        # ìµœì¢… ì‘ë‹µ êµ¬ì„±
        response_message = ResponseMessageOutput(content=cleaned_json_string)
        choice = ChatCompletionChoiceOutput(message=response_message, finish_reason=completion.choices[0].finish_reason if completion.choices else "error")
        final_response_obj = ChatCompletionResponseReal(
            id=completion.id if hasattr(completion, 'id') else f"chatcmpl-realmodel-{uuid.uuid4().hex}",
            object=completion.object if hasattr(completion, 'object') else "chat.completion",
            created=completion.created if hasattr(completion, 'created') else int(time.time()),
            model=completion.model if hasattr(completion, 'model') else MODEL_API_NAME,
            choices=[choice]
        )
        return final_response_obj

    except Exception as e: # ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ
        print(f"[Real API] ğŸ’¥ Llama-3 ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        # ì˜¤ë¥˜ ì‘ë‹µ JSON ìƒì„±
        error_content = json.dumps({
            "ì§„ìœ„": "íŒë‹¨ ì˜¤ë¥˜ (ì„œë²„ ë‚´ë¶€ ë¬¸ì œ)",
            "ê·¼ê±°": f"ëª¨ë¸ ì¶”ë¡  ì¤‘ ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "ë¶„ì„": "ì„œë²„ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        }, ensure_ascii=False, indent=2)
        
        response_message = ResponseMessageOutput(content=error_content)
        choice = ChatCompletionChoiceOutput(message=response_message, finish_reason="error")
        return ChatCompletionResponseReal(
            model=request.model or MODEL_API_NAME, 
            choices=[choice]
        )

if __name__ == '__main__':
    import uvicorn
    if llm_model is None: # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì„œë²„ ì‹œì‘ ì•ˆ í•¨
         print("ğŸ”´ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ API ì„œë²„ë¥¼ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. MODEL_PATH ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        print(f"âœ… Real AI API Server (Llama-3: {MODEL_API_NAME})ê°€ http://127.0.0.1:5005 ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        print("API ì—”ë“œí¬ì¸íŠ¸: POST http://127.0.0.1:5005/v1/chat/completions")
        # 0.0.0.0ìœ¼ë¡œ í˜¸ìŠ¤íŠ¸ë¥¼ ì„¤ì •í•˜ë©´ ì™¸ë¶€ì—ì„œë„ ì ‘ì† ê°€ëŠ¥ (Docker ë“± í™˜ê²½ ê³ ë ¤)
        uvicorn.run(app, host='0.0.0.0', port=5005, log_level="info")